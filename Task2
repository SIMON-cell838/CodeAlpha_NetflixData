#Trends 
#number of Titles added per year
Release_Year_Counts = df['Release Year'].value_counts().sort_index()
plt.figure(figsize=(12,12))
sns.lineplot(x=Release_Year_Counts.index, y=Release_Year_Counts.values)
plt.title('Number of Netflix Title Released Per Year')
plt.xlabel('Release Year')
plt.ylabel('Number Of Titles')
plt.grid(True)
plt.show()

#Patterns
#Distribution of Content Types on Netflix

plt.figure(figsize=(8,8))
df['Content Type'].value_counts().plot.pie(autopct='%1.1f%%',startangle=90,colors=['skyblue','lightcoral'])
plt.title("Distribution of Content Types on Netflix")
plt.ylabel('')
plt.show()

#Anomalies
#Hours Viewed

top_viewed_titles = df.sort_values(by='Hours Viewed', ascending=False).head(10)
print("\nTop 10 Titles by Hours Viewed (Potential Anomalies/Hits):")
print(top_viewed_titles[['Title', 'Content Type', 'Release Date', 'Hours Viewed']])
current_year = pd.Timestamp.now().year
recent_low_performers = df[(df['Release Year'] <= current_year - 5) & (df['Hours Viewed'] < 300000)] #***
print("\nRecent Titles with Low Hours Viewed (Potential Underperformers/Anomalies):")
print(recent_low_performers[['Title', 'Content Type', 'Release Date', 'Hours Viewed']].head())
total_low_hours_titles = len(recent_low_performers)
#total_low_hours_titles = recent_low_performers.shape[0]
print("Total Number of low Hours Viewed Titles: ",total_low_hours_titles)

#patterns
#Distribution of Content by Language Indicator
plt.figure(figsize=(10, 7))
sns.countplot(data=df, y='Language Indicator', order=df['Language Indicator'].value_counts().index,palette='viridis')
plt.title('Overall Distribution of Content by Language Indicator')
plt.xlabel('Number of Titles')
plt.ylabel('Language Indicator')
plt.tight_layout()
plt.show()
# To see the exact counts:
print("\nTop 10 Languages by Number of Titles:")
print(df['Language Indicator'].value_counts().head(10))

#Hypothesis

print("\n--- Testing Hypothesis 1: TV Shows generate more hours viewed than Movies ---")

# Calculate total hours viewed per Content Type
total_hours_by_type = df.groupby('Content Type')['Hours Viewed'].sum()
print("\nTotal Hours Viewed by Content Type:")
print(total_hours_by_type)

plt.figure(figsize=(8, 6))
sns.boxplot(x='Content Type', y='Hours Viewed', data=df)
plt.title('Distribution of Hours Viewed by Content Type')
plt.xlabel('Content Type')
plt.ylabel('Hours Viewed')
plt.yscale('log') # Use log scale if hours viewed vary wildly
plt.show()

# Perform Mann-Whitney U test (more robust for non-normal, skewed data like viewing hours)
movies_hours = df[df['Content Type'] == 'Movie']['Hours Viewed']
tv_shows_hours = df[df['Content Type'] == 'TV Show']['Hours Viewed']

if not movies_hours.empty and not tv_shows_hours.empty:
    stat, p = stats.mannwhitneyu(tv_shows_hours, movies_hours, alternative='greater') # Testing if TV shows are 'greater'
    print(f"\nMann-Whitney U Test:")
    print(f"Statistic = {stat:.2f}, P-value = {p:.3e}") # Print p-value in scientific notation

    alpha = 0.05 # Significance level
    if p < alpha:
        print(f"Validation: Reject null hypothesis. TV Shows statistically generate more hours viewed than Movies (p < {alpha}).")
    else:
        print(f"Validation: Fail to reject null hypothesis. No statistically significant evidence that TV Shows generate more hours viewed than Movies (p >= {alpha}).")
else:
    print("Validation: Not enough data for one or both content types to perform statistical test.")

print("\n--- Testing Hypothesis 2: Non-English content growing in popularity ---")

# Define English vs. Non-English (assuming 'English' is the indicator for English)
df['Is English'] = df['Language Indicator'].apply(lambda x: 'English' if x == 'English' else 'Non-English')

# Group by Release Year and Is English, sum Hours Viewed
language_popularity_yearly = df.groupby(['Release Year', 'Is English'])['Hours Viewed'].sum().unstack(fill_value=0)

# Normalize to show proportion if total content size changes drastically
language_popularity_yearly_norm = language_popularity_yearly.div(language_popularity_yearly.sum(axis=1), axis=0)

# Plot absolute hours viewed trend
plt.figure(figsize=(12, 7))
sns.lineplot(data=language_popularity_yearly, markers=True)
plt.title('Total Hours Viewed by English vs. Non-English Content Per Year')
plt.xlabel('Release Year')
plt.ylabel('Total Hours Viewed')
plt.legend(title='Language Category')
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

# Plot proportional trend
plt.figure(figsize=(12, 7))
sns.lineplot(data=language_popularity_yearly_norm, markers=True)
plt.title('Proportion of Total Hours Viewed by English vs. Non-English Content Per Year')
plt.xlabel('Release Year')
plt.ylabel('Proportion of Total Hours Viewed')
plt.legend(title='Language Category')
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

# Validate assumption: Check recent trend
# Get the trend for Non-English proportion
non_english_trend = language_popularity_yearly_norm['Non-English'].diff().dropna()
if not non_english_trend.empty:
    average_non_english_growth = non_english_trend.mean()
    print(f"Average yearly change in Non-English content's proportion of hours viewed: {average_non_english_growth:.4f}")

    if average_non_english_growth > 0:
        print("Validation: The data suggests a positive trend for Non-English content's popularity.")
    else:
        print("Validation: The data does NOT strongly suggest a positive trend for Non-English content's popularity.")
else:
    print("Validation: Not enough data points to determine a trend for Non-English content proportion.")

df.drop(columns=['Is English'], inplace=True) # Clean up temporary column
